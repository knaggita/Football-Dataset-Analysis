{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "%matplotlib inline \n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from math import isclose\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events = pd.read_csv(\"events.csv\")\n",
    "df_game_info = pd.read_csv(\"ginf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat training and testing labels number of goals in home and away match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10112, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_game_info_select=df_game_info[['fthg','ftag']].fillna(-1)\n",
    "l1=np.array(df_game_info_select.values, dtype=int)\n",
    "l1=l1\n",
    "l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding from labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10112, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_l1=np.empty((l1.shape[0],2))\n",
    "for j in range (l1.shape[0]):\n",
    "        coded_l1[j,0]=int(np.binary_repr(l1[j,0]))\n",
    "        coded_l1[j,1]=int(np.binary_repr(l1[j,1]))\n",
    "coded_l1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select features from events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_select=df_events[['event_type','event_type2','shot_place','shot_outcome','location','side']]\n",
    "#put NAn = -1\n",
    "df_events_select=df_events_select.fillna(-1)\n",
    "colums=['id_odsp','event_type','event_type2','shot_place','shot_outcome','location','side',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert from pandas to numpy arry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941009, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_np=df_events_select.values\n",
    "event_np=np.array(event_np,dtype=int)\n",
    "event_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pne-hot encoding events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941009, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_events=np.ones((event_np.shape[0],6))\n",
    "for i in range (event_np.shape[0]):\n",
    "    for j in range (event_np.shape[1]):\n",
    "        coded_events[i,j]=int(np.binary_repr(event_np[i,j]))\n",
    "coded_events.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create event id to connect event-data with info-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941009, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1=df_events['id_odsp']\n",
    "c1=(c1.values).reshape(-1,1)\n",
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941009, 7)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full=np.concatenate((c1,event_np),axis=1)\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['UFot0hit/', 1.0, 1100.0, ..., 10.0, 1001.0, 10.0],\n",
       "       ['UFot0hit/', 10.0, -1.0, ..., -1.0, -1.0, 1.0],\n",
       "       ['UFot0hit/', 10.0, -1.0, ..., -1.0, -1.0, 1.0],\n",
       "       ...,\n",
       "       ['z5L2OT5E/', 1.0, 1100.0, ..., 10.0, 1001.0, 10.0],\n",
       "       ['z5L2OT5E/', 1000.0, -1.0, ..., -1.0, 100.0, 1.0],\n",
       "       ['z5L2OT5E/', 11.0, -1.0, ..., -1.0, -1.0, 10.0]], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_full=np.concatenate((c1,coded_events),axis=1)\n",
    "coded_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide events to home and away numpy arrries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_home.shape (488224, 6)\n",
      "full_away.shape (452785, 6)\n"
     ]
    }
   ],
   "source": [
    "full_home=[]\n",
    "full_away=[]\n",
    "for i in range (full.shape[0]):\n",
    "    if full[i,6]==1:\n",
    "        full_home.append(full[i])\n",
    "    if full[i,6]==2:\n",
    "        full_away.append(full[i])\n",
    "full_home=np.array(full_home)[:,:6]\n",
    "full_away=np.array(full_away)[:,:6]\n",
    "print('full_home.shape',full_home.shape)\n",
    "print('full_away.shape',full_away.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### divide codedevents to home and away numpy arrries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coded_full_home.shape (488224, 6)\n",
      "coded_full_away.shape (452785, 6)\n"
     ]
    }
   ],
   "source": [
    "coded_full_home=[]\n",
    "coded_full_away=[]\n",
    "for i in range (coded_full.shape[0]):\n",
    "    if coded_full[i,6]==1:\n",
    "        coded_full_home.append(coded_full[i])\n",
    "    if coded_full[i,6]==10:\n",
    "        coded_full_away.append(coded_full[i])\n",
    "coded_full_home=np.array(coded_full_home)[:,:6]\n",
    "coded_full_away=np.array(coded_full_away)[:,:6]\n",
    "print('coded_full_home.shape',coded_full_home.shape)\n",
    "print('coded_full_away.shape',coded_full_away.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create 3-d numpy array (match,events,events values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3-d numpy array to home matches events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9074, 180, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=-1*np.ones((9074,180,5),dtype=int) #180 maxumim number of events per match\n",
    "j=0\n",
    "c=0\n",
    "for i in range (full_home.shape[0]-1):\n",
    "    if full_home[i,0]==full_home[i+1,0]:\n",
    "        a3[c,j]=full_home[i,1:]\n",
    "        j+=1\n",
    "    else:\n",
    "        j=0\n",
    "        c+=1\n",
    "print('c',c)\n",
    "full1=a3\n",
    "full1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3-d numpy array to away matches events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9074, 180, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=-1*np.ones((9074,180,5),dtype=int) #180 maxumim number of events per match\n",
    "j=0\n",
    "c1=0\n",
    "for i in range (full_away.shape[0]-1):\n",
    "    if full_away[i,0]==full_away[i+1,0]:\n",
    "        a3[c1,j]=full_away[i,1:]\n",
    "        j+=1\n",
    "    else:\n",
    "        j=0\n",
    "        c1+=1\n",
    "print('c1',c1)\n",
    "full2=a3\n",
    "full2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3-d numpy array to  coded home matches events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9074, 180, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=-1*np.ones((9074,180,5),dtype=int) #180 maxumim number of events per match\n",
    "j=0\n",
    "c=0\n",
    "for i in range (coded_full_home.shape[0]-1):\n",
    "    if coded_full_home[i,0]==coded_full_home[i+1,0]:\n",
    "        a3[c,j]=coded_full_home[i,1:]\n",
    "        j+=1\n",
    "    else:\n",
    "        j=0\n",
    "        c+=1\n",
    "print('c',c)\n",
    "full3=a3\n",
    "full3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3-d numpy array to  coded away matches events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c 9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9074, 180, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3=-1*np.ones((9074,180,5),dtype=int) #180 maxumim number of events per match\n",
    "j=0\n",
    "c=0\n",
    "for i in range (coded_full_away.shape[0]-1):\n",
    "    if coded_full_away[i,0]==coded_full_away[i+1,0]:\n",
    "        a3[c,j]=coded_full_away[i,1:]\n",
    "        j+=1\n",
    "    else:\n",
    "        j=0\n",
    "        c+=1\n",
    "print('c',c)\n",
    "full4=a3\n",
    "full4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorize data from (180*5) to (1,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9074, 900)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data1=np.empty((9074,900))\n",
    "for i in range (full1.shape[0]):\n",
    "    training_data1[i]=full1[i].reshape(-1) \n",
    "training_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9074, 900)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data2=np.empty((9074,900))\n",
    "for i in range (full2.shape[0]):\n",
    "    training_data2[i]=full2[i].reshape(-1) \n",
    "training_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9074, 900)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data3=np.empty((9074,900))\n",
    "for i in range (full3.shape[0]):\n",
    "    training_data3[i]=full3[i].reshape(-1) \n",
    "training_data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9074, 900)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data4=np.empty((9074,900))\n",
    "for i in range (full4.shape[0]):\n",
    "    training_data4[i]=full4[i].reshape(-1) \n",
    "training_data4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function using RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the random seed for reproducible results.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # This just calls the base class constructor.\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_gen,labels, criterion, optimizer):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # Set the model to training mode. This will turn on layers that would\n",
    "    # otherwise behave differently during evaluation, such as dropout.\n",
    "    model.train()\n",
    "    # Store the number of sequences that were classified correctly.\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
    "    # is defined as the number of batches required to produce a total of roughly 1000\n",
    "    # sequences given a batch size.\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "        # For each new batch, clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data_gen[batch_idx],labels[batch_idx]\n",
    "        data=data.view(1,1,900)\n",
    "        target=target.view(1,1,1)\n",
    "#         print('data.shape',data.shape)\n",
    "#         print('target.shape',target.shape)\n",
    "        data, target = data.float().to(device), target.long().to(device)\n",
    "        \n",
    "        # Perform the forward pass of the model.\n",
    "        output = model(data)\n",
    "        loss = criterion(output.float(), target.float())\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = output\n",
    "        if abs(y_pred - target.float()) <= 0.5:\n",
    "            num_correct+=1\n",
    "#         num_correct += (y_pred == target.float()).int().sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_gen,labels_test, criterion):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # Set the model to evaluation mode. This will turn off layers that would\n",
    "    # otherwise behave differently during training, such as dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Store the number of sequences that were classified correctly.\n",
    "    num_correct = 0\n",
    "\n",
    "    # A context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx],labels_test[batch_idx]\n",
    "#             print('test taget',target)\n",
    "            data=data.view(1,1,900)\n",
    "            target=target.view(1,1,1)\n",
    "            \n",
    "            data, target = (data).float().to(device), (target).long().to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output.float(), target.float())\n",
    "            y_pred = output\n",
    "            if abs(y_pred - target.float()) <= 0.5:\n",
    "                num_correct+=1\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_data_gen,labels, test_data_gen,labels_test, criterion, optimizer, max_epochs, verbose=True):\n",
    "    # Automatically determine the device that PyTorch should use for computation.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    text_file_loss = open(\"loss_away_one_hot.txt\", \"w\")\n",
    "    text_file_acc = open(\"acc_away_one_hot.txt\", \"w\")\n",
    "    # Track the value of the loss function and model accuracy across epochs.\n",
    "    history_train = {'loss': [], 'acc': []}\n",
    "    history_test = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        num_correct, loss = train(model, train_data_gen,labels, criterion, optimizer)\n",
    "#         accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n",
    "        accuracy = float(num_correct) / (train_data_gen.shape[0]) * 100\n",
    "        history_train['loss'].append(loss)\n",
    "        history_train['acc'].append(accuracy)\n",
    "        \n",
    "        # Do the same for the testing loop.\n",
    "        num_correct, loss = test(model, test_data_gen,labels_test, criterion)\n",
    "        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen) * 100\n",
    "#         accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.numpy().batch_size) * 100\n",
    "        history_test['loss'].append(loss)\n",
    "        history_test['acc'].append(accuracy)\n",
    "\n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            text_file_loss.write('{} , '.format(history_train['loss'][-1]))\n",
    "            text_file_acc.write('{} , '.format(history_train['acc'][-1]))\n",
    "            print('[Epoch {} / {}] loss: {}, acc {}%'.format(epoch + 1,max_epochs,history_train['loss'][-1],history_train['acc'][-1]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train one-hot encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### home without one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_d.shape torch.Size([2000, 900])\n",
      "train_label.shape torch.Size([2000, 1])\n",
      "test_d.shape torch.Size([100, 900])\n",
      "test_label.shape torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_d=training_data1[0:2000]\n",
    "# train_d=np,atrain_d,\n",
    "train_d=torch.from_numpy(train_d)\n",
    "print('train_d.shape',train_d.shape)\n",
    "train_label=l1[0:2000,0].reshape(-1,1)\n",
    "# np.concatenate((),axis=1)\n",
    "train_label=torch.from_numpy(train_label)\n",
    "print('train_label.shape',train_label.shape)\n",
    "test_d=training_data1[2000:2100]\n",
    "test_d=torch.from_numpy(test_d)\n",
    "print('test_d.shape',test_d.shape)\n",
    "test_label=l1[2000:2100,0]\n",
    "test_label=torch.from_numpy(test_label)\n",
    "print('test_label.shape',test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 100] loss: 2.213989496231079, acc 25.8%\n",
      "[Epoch 2 / 100] loss: 2.2713334560394287, acc 28.999999999999996%\n",
      "[Epoch 3 / 100] loss: 1.7270112037658691, acc 30.0%\n",
      "[Epoch 4 / 100] loss: 0.44491833448410034, acc 31.05%\n",
      "[Epoch 5 / 100] loss: 1.29188072681427, acc 34.25%\n",
      "[Epoch 6 / 100] loss: 0.2968190312385559, acc 33.800000000000004%\n",
      "[Epoch 7 / 100] loss: 0.1385488212108612, acc 35.25%\n",
      "[Epoch 8 / 100] loss: 0.32342100143432617, acc 35.449999999999996%\n",
      "[Epoch 9 / 100] loss: 0.2702680230140686, acc 37.55%\n",
      "[Epoch 10 / 100] loss: 0.47118890285491943, acc 38.85%\n",
      "[Epoch 11 / 100] loss: 0.04121921584010124, acc 39.35%\n",
      "[Epoch 12 / 100] loss: 0.12789642810821533, acc 40.699999999999996%\n",
      "[Epoch 13 / 100] loss: 0.7619878053665161, acc 42.65%\n",
      "[Epoch 14 / 100] loss: 1.2442492246627808, acc 45.6%\n",
      "[Epoch 15 / 100] loss: 0.172923281788826, acc 46.5%\n",
      "[Epoch 16 / 100] loss: 0.7130605578422546, acc 46.550000000000004%\n",
      "[Epoch 17 / 100] loss: 0.2540840804576874, acc 46.25%\n",
      "[Epoch 18 / 100] loss: 1.1361963748931885, acc 48.55%\n",
      "[Epoch 19 / 100] loss: 0.9625819325447083, acc 48.4%\n",
      "[Epoch 20 / 100] loss: 0.3191279470920563, acc 47.75%\n",
      "[Epoch 21 / 100] loss: 0.31852397322654724, acc 46.75%\n",
      "[Epoch 22 / 100] loss: 0.5726369619369507, acc 47.85%\n",
      "[Epoch 23 / 100] loss: 0.2677219808101654, acc 48.65%\n",
      "[Epoch 24 / 100] loss: 0.30505192279815674, acc 52.75%\n",
      "[Epoch 25 / 100] loss: 0.3649124503135681, acc 54.50000000000001%\n",
      "[Epoch 26 / 100] loss: 0.30043333768844604, acc 52.15%\n",
      "[Epoch 27 / 100] loss: 0.304683119058609, acc 52.300000000000004%\n",
      "[Epoch 28 / 100] loss: 0.08750994503498077, acc 53.400000000000006%\n",
      "[Epoch 29 / 100] loss: 0.2766256034374237, acc 53.75%\n",
      "[Epoch 30 / 100] loss: 0.40381649136543274, acc 53.800000000000004%\n",
      "[Epoch 31 / 100] loss: 0.23998874425888062, acc 53.300000000000004%\n",
      "[Epoch 32 / 100] loss: 0.37509748339653015, acc 56.49999999999999%\n",
      "[Epoch 33 / 100] loss: 0.23620639741420746, acc 58.95%\n",
      "[Epoch 34 / 100] loss: 0.25258389115333557, acc 55.45%\n",
      "[Epoch 35 / 100] loss: 0.7005462646484375, acc 56.99999999999999%\n",
      "[Epoch 36 / 100] loss: 0.31350836157798767, acc 53.449999999999996%\n",
      "[Epoch 37 / 100] loss: 0.34268930554389954, acc 55.35%\n",
      "[Epoch 38 / 100] loss: 0.20862723886966705, acc 57.8%\n",
      "[Epoch 39 / 100] loss: 0.7609736919403076, acc 57.65%\n",
      "[Epoch 40 / 100] loss: 0.08147570490837097, acc 58.15%\n",
      "[Epoch 41 / 100] loss: 0.3178475499153137, acc 55.900000000000006%\n",
      "[Epoch 42 / 100] loss: 0.05920308083295822, acc 58.25%\n",
      "[Epoch 43 / 100] loss: 0.3137173652648926, acc 56.25%\n",
      "[Epoch 44 / 100] loss: 0.7884337306022644, acc 57.99999999999999%\n",
      "[Epoch 45 / 100] loss: 0.33249664306640625, acc 58.25%\n",
      "[Epoch 46 / 100] loss: 0.71123868227005, acc 57.35%\n",
      "[Epoch 47 / 100] loss: 0.3570376932621002, acc 57.8%\n",
      "[Epoch 48 / 100] loss: 0.3651878833770752, acc 56.45%\n",
      "[Epoch 49 / 100] loss: 0.34870466589927673, acc 58.75%\n",
      "[Epoch 50 / 100] loss: 0.46515268087387085, acc 61.150000000000006%\n",
      "[Epoch 51 / 100] loss: 0.7530592679977417, acc 59.3%\n",
      "[Epoch 52 / 100] loss: 0.33191564679145813, acc 59.699999999999996%\n",
      "[Epoch 53 / 100] loss: 0.4442582130432129, acc 60.6%\n",
      "[Epoch 54 / 100] loss: 0.29753831028938293, acc 58.099999999999994%\n",
      "[Epoch 55 / 100] loss: 0.3139798939228058, acc 61.35%\n",
      "[Epoch 56 / 100] loss: 0.41916289925575256, acc 59.95%\n",
      "[Epoch 57 / 100] loss: 0.3031248450279236, acc 59.050000000000004%\n",
      "[Epoch 58 / 100] loss: 0.3154023587703705, acc 61.199999999999996%\n",
      "[Epoch 59 / 100] loss: 0.3087148666381836, acc 59.099999999999994%\n",
      "[Epoch 60 / 100] loss: 0.3028336465358734, acc 60.8%\n",
      "[Epoch 61 / 100] loss: 0.2536836862564087, acc 58.9%\n",
      "[Epoch 62 / 100] loss: 0.2836986780166626, acc 59.5%\n",
      "[Epoch 63 / 100] loss: 0.267922043800354, acc 63.5%\n",
      "[Epoch 64 / 100] loss: 0.08737976104021072, acc 62.0%\n",
      "[Epoch 65 / 100] loss: 0.234561488032341, acc 59.35%\n",
      "[Epoch 66 / 100] loss: 0.2242995798587799, acc 63.6%\n",
      "[Epoch 67 / 100] loss: 0.22831355035305023, acc 58.45%\n",
      "[Epoch 68 / 100] loss: 0.17329919338226318, acc 61.45%\n",
      "[Epoch 69 / 100] loss: 0.2175293117761612, acc 62.6%\n",
      "[Epoch 70 / 100] loss: 0.22148019075393677, acc 62.4%\n",
      "[Epoch 71 / 100] loss: 0.23852373659610748, acc 63.14999999999999%\n",
      "[Epoch 72 / 100] loss: 1.3721522092819214, acc 58.699999999999996%\n",
      "[Epoch 73 / 100] loss: 0.20133204758167267, acc 62.25000000000001%\n",
      "[Epoch 74 / 100] loss: 0.19768820703029633, acc 62.8%\n",
      "[Epoch 75 / 100] loss: 0.14694108068943024, acc 59.4%\n",
      "[Epoch 76 / 100] loss: 0.14845627546310425, acc 63.2%\n",
      "[Epoch 77 / 100] loss: 0.18988877534866333, acc 62.55%\n",
      "[Epoch 78 / 100] loss: 0.15755797922611237, acc 64.8%\n",
      "[Epoch 79 / 100] loss: 0.16705560684204102, acc 64.35%\n",
      "[Epoch 80 / 100] loss: 0.1553463488817215, acc 64.3%\n",
      "[Epoch 81 / 100] loss: 0.13940034806728363, acc 63.0%\n",
      "[Epoch 82 / 100] loss: 0.14774568378925323, acc 65.95%\n",
      "[Epoch 83 / 100] loss: 0.31373801827430725, acc 65.5%\n",
      "[Epoch 84 / 100] loss: 0.18768571317195892, acc 66.45%\n",
      "[Epoch 85 / 100] loss: 0.1609538495540619, acc 64.35%\n",
      "[Epoch 86 / 100] loss: 0.1502799242734909, acc 67.9%\n",
      "[Epoch 87 / 100] loss: 0.2157629430294037, acc 64.1%\n",
      "[Epoch 88 / 100] loss: 0.16087624430656433, acc 65.9%\n",
      "[Epoch 89 / 100] loss: 0.32201728224754333, acc 66.4%\n",
      "[Epoch 90 / 100] loss: 0.12332797050476074, acc 66.25%\n",
      "[Epoch 91 / 100] loss: 0.14182744920253754, acc 64.45%\n",
      "[Epoch 92 / 100] loss: 0.13937370479106903, acc 64.14999999999999%\n",
      "[Epoch 93 / 100] loss: 0.14549803733825684, acc 68.5%\n",
      "[Epoch 94 / 100] loss: 0.13153932988643646, acc 66.9%\n",
      "[Epoch 95 / 100] loss: 0.13768810033798218, acc 67.2%\n",
      "[Epoch 96 / 100] loss: 0.10590659081935883, acc 67.2%\n",
      "[Epoch 97 / 100] loss: 0.02342037670314312, acc 66.9%\n",
      "[Epoch 98 / 100] loss: 0.12119510769844055, acc 66.60000000000001%\n",
      "[Epoch 99 / 100] loss: 0.022589420899748802, acc 67.95%\n",
      "[Epoch 100 / 100] loss: 0.04556592181324959, acc 69.25%\n",
      "SimpleRNN(\n",
      "  (rnn): RNN(900, 500, batch_first=True)\n",
      "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Total time is 4984.555050394993 Second\n"
     ]
    }
   ],
   "source": [
    "# Setup the training and test data generators.\n",
    "batch_size     = 100\n",
    "train_data_gen = train_d\n",
    "test_data_gen  = test_d\n",
    "labels   =     train_label\n",
    "labels_test=test_label\n",
    "# print('train_data_gen.shape',train_data_gen.shape)\n",
    "# print('labels.shape',labels.shape)\n",
    "# Setup the RNN and training settings.\n",
    "input_size  = 900\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.MSELoss()\n",
    "# optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer    =torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-3)\n",
    "max_epochs  = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model.\n",
    "start=time.perf_counter()\n",
    "model = train_and_test(model, train_data_gen,labels, test_data_gen,labels_test, criterion, optimizer, max_epochs)\n",
    "# model=train(model, train_data_gen,labels, criterion, optimizer)\n",
    "print(model)\n",
    "print('Total time is {} Second'.format(time.perf_counter()-start))\n",
    "#torch.save(model,'without_one_hot_encoding_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### away without one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_d.shape torch.Size([2000, 900])\n",
      "train_label.shape torch.Size([2000, 1])\n",
      "test_d.shape torch.Size([100, 900])\n",
      "test_label.shape torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_d=training_data2[0:2000]\n",
    "# train_d=np,atrain_d,\n",
    "train_d=torch.from_numpy(train_d)\n",
    "print('train_d.shape',train_d.shape)\n",
    "train_label=l1[0:2000,1].reshape(-1,1)\n",
    "# np.concatenate((),axis=1)\n",
    "train_label=torch.from_numpy(train_label)\n",
    "print('train_label.shape',train_label.shape)\n",
    "test_d=training_data2[2000:2100]\n",
    "test_d=torch.from_numpy(test_d)\n",
    "print('test_d.shape',test_d.shape)\n",
    "test_label=l1[2000:2100,1]\n",
    "test_label=torch.from_numpy(test_label)\n",
    "print('test_label.shape',test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 100] loss: 0.22605489194393158, acc 31.25%\n",
      "[Epoch 2 / 100] loss: 0.007035145536065102, acc 32.15%\n",
      "[Epoch 3 / 100] loss: 0.0006912393146194518, acc 33.2%\n",
      "[Epoch 4 / 100] loss: 0.003385464195162058, acc 34.8%\n",
      "[Epoch 5 / 100] loss: 0.37672752141952515, acc 37.15%\n",
      "[Epoch 6 / 100] loss: 0.005100299138575792, acc 35.35%\n",
      "[Epoch 7 / 100] loss: 0.36848849058151245, acc 37.5%\n",
      "[Epoch 8 / 100] loss: 0.016117004677653313, acc 39.45%\n",
      "[Epoch 9 / 100] loss: 0.004214274697005749, acc 39.75%\n",
      "[Epoch 10 / 100] loss: 4.060683750140015e-06, acc 39.900000000000006%\n",
      "[Epoch 11 / 100] loss: 0.02343786135315895, acc 40.849999999999994%\n",
      "[Epoch 12 / 100] loss: 0.1439531445503235, acc 42.15%\n",
      "[Epoch 13 / 100] loss: 0.029528601095080376, acc 42.35%\n",
      "[Epoch 14 / 100] loss: 0.028509659692645073, acc 42.199999999999996%\n",
      "[Epoch 15 / 100] loss: 0.08163485676050186, acc 44.0%\n",
      "[Epoch 16 / 100] loss: 0.03109005093574524, acc 45.300000000000004%\n",
      "[Epoch 17 / 100] loss: 0.0030980631709098816, acc 47.55%\n",
      "[Epoch 18 / 100] loss: 0.2161707729101181, acc 45.35%\n",
      "[Epoch 19 / 100] loss: 0.0062286872416734695, acc 49.15%\n",
      "[Epoch 20 / 100] loss: 0.023175213485956192, acc 49.05%\n",
      "[Epoch 21 / 100] loss: 0.23769275844097137, acc 49.6%\n",
      "[Epoch 22 / 100] loss: 0.04915546625852585, acc 49.75%\n",
      "[Epoch 23 / 100] loss: 0.01873004250228405, acc 51.449999999999996%\n",
      "[Epoch 24 / 100] loss: 0.1377241462469101, acc 51.24999999999999%\n",
      "[Epoch 25 / 100] loss: 0.012506761588156223, acc 51.800000000000004%\n",
      "[Epoch 26 / 100] loss: 0.33849892020225525, acc 54.400000000000006%\n",
      "[Epoch 27 / 100] loss: 0.1694796234369278, acc 55.55%\n",
      "[Epoch 28 / 100] loss: 0.14092601835727692, acc 55.65%\n",
      "[Epoch 29 / 100] loss: 0.4012633264064789, acc 57.25%\n",
      "[Epoch 30 / 100] loss: 0.07003375142812729, acc 56.85%\n",
      "[Epoch 31 / 100] loss: 0.38139134645462036, acc 57.15%\n",
      "[Epoch 32 / 100] loss: 0.0008050340693444014, acc 57.699999999999996%\n",
      "[Epoch 33 / 100] loss: 0.3025498390197754, acc 61.199999999999996%\n",
      "[Epoch 34 / 100] loss: 0.03876524791121483, acc 58.15%\n",
      "[Epoch 35 / 100] loss: 0.03109211102128029, acc 59.3%\n",
      "[Epoch 36 / 100] loss: 3.9547088817926124e-05, acc 61.95%\n",
      "[Epoch 37 / 100] loss: 0.3951994776725769, acc 61.150000000000006%\n",
      "[Epoch 38 / 100] loss: 0.9960166811943054, acc 61.0%\n",
      "[Epoch 39 / 100] loss: 0.011005223728716373, acc 63.949999999999996%\n",
      "[Epoch 40 / 100] loss: 0.0013383381301537156, acc 63.74999999999999%\n",
      "[Epoch 41 / 100] loss: 0.010404680855572224, acc 62.2%\n",
      "[Epoch 42 / 100] loss: 0.3302702307701111, acc 64.45%\n",
      "[Epoch 43 / 100] loss: 0.28749880194664, acc 64.64999999999999%\n",
      "[Epoch 44 / 100] loss: 0.16952762007713318, acc 63.7%\n",
      "[Epoch 45 / 100] loss: 0.21214501559734344, acc 64.4%\n",
      "[Epoch 46 / 100] loss: 0.14332766830921173, acc 63.800000000000004%\n",
      "[Epoch 47 / 100] loss: 0.30319008231163025, acc 65.60000000000001%\n",
      "[Epoch 48 / 100] loss: 0.00432405062019825, acc 64.05%\n",
      "[Epoch 49 / 100] loss: 0.05221407860517502, acc 65.2%\n",
      "[Epoch 50 / 100] loss: 0.2335437834262848, acc 62.9%\n",
      "[Epoch 51 / 100] loss: 0.31165578961372375, acc 67.15%\n",
      "[Epoch 52 / 100] loss: 0.04145705699920654, acc 64.9%\n",
      "[Epoch 53 / 100] loss: 0.0008050070027820766, acc 66.05%\n",
      "[Epoch 54 / 100] loss: 0.28876006603240967, acc 67.85%\n",
      "[Epoch 55 / 100] loss: 0.06054992973804474, acc 66.10000000000001%\n",
      "[Epoch 56 / 100] loss: 0.1423231065273285, acc 69.5%\n",
      "[Epoch 57 / 100] loss: 0.20094889402389526, acc 67.85%\n",
      "[Epoch 58 / 100] loss: 0.34737709164619446, acc 69.45%\n",
      "[Epoch 59 / 100] loss: 0.24215009808540344, acc 65.5%\n",
      "[Epoch 60 / 100] loss: 0.15136857330799103, acc 69.5%\n",
      "[Epoch 61 / 100] loss: 0.012924742884933949, acc 68.15%\n",
      "[Epoch 62 / 100] loss: 0.080553337931633, acc 69.75%\n",
      "[Epoch 63 / 100] loss: 0.5656960010528564, acc 69.85%\n",
      "[Epoch 64 / 100] loss: 0.4140523374080658, acc 69.0%\n",
      "[Epoch 65 / 100] loss: 0.2352890819311142, acc 68.89999999999999%\n",
      "[Epoch 66 / 100] loss: 0.00631633959710598, acc 70.3%\n",
      "[Epoch 67 / 100] loss: 0.5824187994003296, acc 70.0%\n",
      "[Epoch 68 / 100] loss: 0.16579386591911316, acc 71.6%\n",
      "[Epoch 69 / 100] loss: 0.06305956095457077, acc 70.1%\n",
      "[Epoch 70 / 100] loss: 0.12661665678024292, acc 71.5%\n",
      "[Epoch 71 / 100] loss: 0.09690845757722855, acc 67.95%\n",
      "[Epoch 72 / 100] loss: 1.2830381393432617, acc 68.2%\n",
      "[Epoch 73 / 100] loss: 0.49995049834251404, acc 66.9%\n",
      "[Epoch 74 / 100] loss: 0.7215077877044678, acc 69.39999999999999%\n",
      "[Epoch 75 / 100] loss: 0.6471810936927795, acc 71.1%\n",
      "[Epoch 76 / 100] loss: 0.7775042653083801, acc 72.8%\n",
      "[Epoch 77 / 100] loss: 0.6701285243034363, acc 72.75%\n",
      "[Epoch 78 / 100] loss: 0.6826739311218262, acc 70.55%\n",
      "[Epoch 79 / 100] loss: 0.5951833724975586, acc 70.65%\n",
      "[Epoch 80 / 100] loss: 0.654069185256958, acc 73.25%\n",
      "[Epoch 81 / 100] loss: 0.6749749779701233, acc 71.1%\n",
      "[Epoch 82 / 100] loss: 0.5974013209342957, acc 72.8%\n",
      "[Epoch 83 / 100] loss: 0.6652919054031372, acc 74.0%\n",
      "[Epoch 84 / 100] loss: 0.7699694037437439, acc 73.35000000000001%\n",
      "[Epoch 85 / 100] loss: 0.5554596185684204, acc 68.2%\n",
      "[Epoch 86 / 100] loss: 1.0073620080947876, acc 73.45%\n",
      "[Epoch 87 / 100] loss: 1.0147241353988647, acc 71.15%\n",
      "[Epoch 88 / 100] loss: 0.5174262523651123, acc 70.05%\n",
      "[Epoch 89 / 100] loss: 0.31780704855918884, acc 71.7%\n",
      "[Epoch 90 / 100] loss: 0.6883091330528259, acc 73.25%\n",
      "[Epoch 91 / 100] loss: 0.24446693062782288, acc 68.15%\n",
      "[Epoch 92 / 100] loss: 0.5846684575080872, acc 72.39999999999999%\n",
      "[Epoch 93 / 100] loss: 0.4638543426990509, acc 69.85%\n",
      "[Epoch 94 / 100] loss: 0.21061845123767853, acc 71.0%\n",
      "[Epoch 95 / 100] loss: 0.10015871375799179, acc 69.05%\n",
      "[Epoch 96 / 100] loss: 0.179779514670372, acc 72.65%\n",
      "[Epoch 97 / 100] loss: 0.1020330861210823, acc 70.19999999999999%\n",
      "[Epoch 98 / 100] loss: 0.15644490718841553, acc 72.6%\n",
      "[Epoch 99 / 100] loss: 0.20459815859794617, acc 71.55%\n",
      "[Epoch 100 / 100] loss: 0.1297169327735901, acc 72.39999999999999%\n",
      "SimpleRNN(\n",
      "  (rnn): RNN(900, 500, batch_first=True)\n",
      "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Total time is 4828.730167474976 Second\n"
     ]
    }
   ],
   "source": [
    "# Setup the training and test data generators.\n",
    "batch_size     = 100\n",
    "train_data_gen = train_d\n",
    "test_data_gen  = test_d\n",
    "labels   =     train_label\n",
    "labels_test=test_label\n",
    "# print('train_data_gen.shape',train_data_gen.shape)\n",
    "# print('labels.shape',labels.shape)\n",
    "# Setup the RNN and training settings.\n",
    "input_size  = 900\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.MSELoss()\n",
    "# optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer    =torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-3)\n",
    "max_epochs  = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model.\n",
    "start=time.perf_counter()\n",
    "model = train_and_test(model, train_data_gen,labels, test_data_gen,labels_test, criterion, optimizer, max_epochs)\n",
    "# model=train(model, train_data_gen,labels, criterion, optimizer)\n",
    "print(model)\n",
    "print('Total time is {} Second'.format(time.perf_counter()-start))\n",
    "#torch.save(model,'without_one_hot_encoding_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### home one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_d.shape torch.Size([2000, 900])\n",
      "train_label.shape torch.Size([2000, 1])\n",
      "test_d.shape torch.Size([100, 900])\n",
      "test_label.shape torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_d=training_data3[0:2000]\n",
    "# train_d=np,atrain_d,\n",
    "train_d=torch.from_numpy(train_d)\n",
    "print('train_d.shape',train_d.shape)\n",
    "train_label=coded_l1[0:2000,0].reshape(-1,1)\n",
    "# np.concatenate((),axis=1)\n",
    "train_label=torch.from_numpy(train_label)\n",
    "print('train_label.shape',train_label.shape)\n",
    "test_d=training_data3[2000:2100]\n",
    "test_d=torch.from_numpy(test_d)\n",
    "print('test_d.shape',test_d.shape)\n",
    "test_label=coded_l1[2000:2100,0]\n",
    "test_label=torch.from_numpy(test_label)\n",
    "print('test_label.shape',test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 100] loss: 48.84050750732422, acc 2.55%\n",
      "[Epoch 2 / 100] loss: 49.902740478515625, acc 3.4000000000000004%\n",
      "[Epoch 3 / 100] loss: 102.30608367919922, acc 4.3999999999999995%\n",
      "[Epoch 4 / 100] loss: 21.406017303466797, acc 7.249999999999999%\n",
      "[Epoch 5 / 100] loss: 44.58956527709961, acc 11.85%\n",
      "[Epoch 6 / 100] loss: 16.115751266479492, acc 15.5%\n",
      "[Epoch 7 / 100] loss: 1.1055079698562622, acc 20.25%\n",
      "[Epoch 8 / 100] loss: 1.7203938961029053, acc 24.0%\n",
      "[Epoch 9 / 100] loss: 2.6252493858337402, acc 16.400000000000002%\n",
      "[Epoch 10 / 100] loss: 3.5374975204467773, acc 0.95%\n",
      "[Epoch 11 / 100] loss: 4.440035343170166, acc 1.0%\n",
      "[Epoch 12 / 100] loss: 5.1578216552734375, acc 0.6%\n",
      "[Epoch 13 / 100] loss: 6.076781749725342, acc 1.05%\n",
      "[Epoch 14 / 100] loss: 6.703622341156006, acc 1.9%\n",
      "[Epoch 15 / 100] loss: 7.669404029846191, acc 1.55%\n",
      "[Epoch 16 / 100] loss: 8.055832862854004, acc 1.0%\n",
      "[Epoch 17 / 100] loss: 8.808653831481934, acc 0.95%\n",
      "[Epoch 18 / 100] loss: 9.318214416503906, acc 0.95%\n",
      "[Epoch 19 / 100] loss: 9.876300811767578, acc 0.35000000000000003%\n",
      "[Epoch 20 / 100] loss: 10.841981887817383, acc 0.44999999999999996%\n",
      "[Epoch 21 / 100] loss: 11.447834014892578, acc 1.0%\n",
      "[Epoch 22 / 100] loss: 12.340420722961426, acc 0.8500000000000001%\n",
      "[Epoch 23 / 100] loss: 13.100565910339355, acc 0.65%\n",
      "[Epoch 24 / 100] loss: 14.242375373840332, acc 0.8999999999999999%\n",
      "[Epoch 25 / 100] loss: 15.132206916809082, acc 0.8500000000000001%\n",
      "[Epoch 26 / 100] loss: 15.995128631591797, acc 1.2%\n",
      "[Epoch 27 / 100] loss: 16.421722412109375, acc 1.0%\n",
      "[Epoch 28 / 100] loss: 16.947498321533203, acc 0.8999999999999999%\n",
      "[Epoch 29 / 100] loss: 17.377174377441406, acc 1.15%\n",
      "[Epoch 30 / 100] loss: 18.003433227539062, acc 0.7000000000000001%\n",
      "[Epoch 31 / 100] loss: 18.449766159057617, acc 0.6%\n",
      "[Epoch 32 / 100] loss: 19.05581283569336, acc 0.6%\n",
      "[Epoch 33 / 100] loss: 19.532611846923828, acc 0.6%\n",
      "[Epoch 34 / 100] loss: 20.041784286499023, acc 0.7000000000000001%\n",
      "[Epoch 35 / 100] loss: 20.492351531982422, acc 0.8%\n",
      "[Epoch 36 / 100] loss: 21.394926071166992, acc 0.8500000000000001%\n",
      "[Epoch 37 / 100] loss: 21.74178695678711, acc 0.8500000000000001%\n",
      "[Epoch 38 / 100] loss: 22.34274673461914, acc 0.5%\n",
      "[Epoch 39 / 100] loss: 22.958784103393555, acc 0.95%\n",
      "[Epoch 40 / 100] loss: 23.579368591308594, acc 0.5499999999999999%\n",
      "[Epoch 41 / 100] loss: 24.28882598876953, acc 0.25%\n",
      "[Epoch 42 / 100] loss: 24.867015838623047, acc 0.75%\n",
      "[Epoch 43 / 100] loss: 25.65999412536621, acc 0.6%\n",
      "[Epoch 44 / 100] loss: 25.74057388305664, acc 0.7000000000000001%\n",
      "[Epoch 45 / 100] loss: 26.133075714111328, acc 0.8500000000000001%\n",
      "[Epoch 46 / 100] loss: 26.733924865722656, acc 0.7000000000000001%\n",
      "[Epoch 47 / 100] loss: 26.991241455078125, acc 0.6%\n",
      "[Epoch 48 / 100] loss: 26.656408309936523, acc 0.8999999999999999%\n",
      "[Epoch 49 / 100] loss: 27.192293167114258, acc 1.2%\n",
      "[Epoch 50 / 100] loss: 27.330150604248047, acc 0.5%\n",
      "[Epoch 51 / 100] loss: 27.391164779663086, acc 0.7000000000000001%\n",
      "[Epoch 52 / 100] loss: 27.884183883666992, acc 0.5%\n",
      "[Epoch 53 / 100] loss: 27.9586181640625, acc 0.5%\n",
      "[Epoch 54 / 100] loss: 28.238142013549805, acc 0.75%\n",
      "[Epoch 55 / 100] loss: 28.91697883605957, acc 1.05%\n",
      "[Epoch 56 / 100] loss: 29.279170989990234, acc 0.7000000000000001%\n",
      "[Epoch 57 / 100] loss: 29.996288299560547, acc 0.8500000000000001%\n",
      "[Epoch 58 / 100] loss: 30.32880210876465, acc 0.5%\n",
      "[Epoch 59 / 100] loss: 30.844013214111328, acc 0.65%\n",
      "[Epoch 60 / 100] loss: 31.221019744873047, acc 0.8%\n",
      "[Epoch 61 / 100] loss: 31.908288955688477, acc 0.65%\n",
      "[Epoch 62 / 100] loss: 32.518707275390625, acc 0.75%\n",
      "[Epoch 63 / 100] loss: 32.676300048828125, acc 0.5%\n",
      "[Epoch 64 / 100] loss: 32.89218521118164, acc 0.35000000000000003%\n",
      "[Epoch 65 / 100] loss: 33.262351989746094, acc 0.6%\n",
      "[Epoch 66 / 100] loss: 33.42402648925781, acc 0.5499999999999999%\n",
      "[Epoch 67 / 100] loss: 33.5877685546875, acc 0.7000000000000001%\n",
      "[Epoch 68 / 100] loss: 33.897987365722656, acc 0.8%\n",
      "[Epoch 69 / 100] loss: 34.129146575927734, acc 0.5%\n",
      "[Epoch 70 / 100] loss: 34.36943435668945, acc 0.44999999999999996%\n",
      "[Epoch 71 / 100] loss: 34.541954040527344, acc 0.6%\n",
      "[Epoch 72 / 100] loss: 34.79174041748047, acc 0.7000000000000001%\n",
      "[Epoch 73 / 100] loss: 34.298648834228516, acc 0.95%\n",
      "[Epoch 74 / 100] loss: 34.562294006347656, acc 0.6%\n",
      "[Epoch 75 / 100] loss: 34.5003776550293, acc 1.0999999999999999%\n",
      "[Epoch 76 / 100] loss: 34.7795295715332, acc 0.8500000000000001%\n",
      "[Epoch 77 / 100] loss: 34.817134857177734, acc 0.75%\n",
      "[Epoch 78 / 100] loss: 35.37933349609375, acc 0.7000000000000001%\n",
      "[Epoch 79 / 100] loss: 35.31890106201172, acc 0.8%\n",
      "[Epoch 80 / 100] loss: 35.57256317138672, acc 0.5%\n",
      "[Epoch 81 / 100] loss: 35.38359832763672, acc 0.6%\n",
      "[Epoch 82 / 100] loss: 35.6970329284668, acc 0.5499999999999999%\n",
      "[Epoch 83 / 100] loss: 36.3093376159668, acc 0.6%\n",
      "[Epoch 84 / 100] loss: 36.26877975463867, acc 0.4%\n",
      "[Epoch 85 / 100] loss: 36.55360794067383, acc 0.35000000000000003%\n",
      "[Epoch 86 / 100] loss: 37.11384201049805, acc 0.5%\n",
      "[Epoch 87 / 100] loss: 37.74921798706055, acc 0.5%\n",
      "[Epoch 88 / 100] loss: 37.97233200073242, acc 1.0%\n",
      "[Epoch 89 / 100] loss: 38.407169342041016, acc 0.7000000000000001%\n",
      "[Epoch 90 / 100] loss: 38.036739349365234, acc 0.65%\n",
      "[Epoch 91 / 100] loss: 38.689796447753906, acc 0.44999999999999996%\n",
      "[Epoch 92 / 100] loss: 38.77217102050781, acc 0.8%\n",
      "[Epoch 93 / 100] loss: 39.43021774291992, acc 0.6%\n",
      "[Epoch 94 / 100] loss: 39.28193283081055, acc 0.6%\n",
      "[Epoch 95 / 100] loss: 39.85419845581055, acc 0.6%\n",
      "[Epoch 96 / 100] loss: 39.21474075317383, acc 0.6%\n",
      "[Epoch 97 / 100] loss: 39.40183639526367, acc 0.65%\n",
      "[Epoch 98 / 100] loss: 39.40562057495117, acc 0.95%\n",
      "[Epoch 99 / 100] loss: 39.653018951416016, acc 0.8500000000000001%\n",
      "[Epoch 100 / 100] loss: 40.54595947265625, acc 0.7000000000000001%\n",
      "SimpleRNN(\n",
      "  (rnn): RNN(900, 500, batch_first=True)\n",
      "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Total time is 5043.573611519998 Second\n"
     ]
    }
   ],
   "source": [
    "# Setup the training and test data generators.\n",
    "batch_size     = 100\n",
    "train_data_gen = train_d\n",
    "test_data_gen  = test_d\n",
    "labels   =     train_label\n",
    "labels_test=test_label\n",
    "# print('train_data_gen.shape',train_data_gen.shape)\n",
    "# print('labels.shape',labels.shape)\n",
    "# Setup the RNN and training settings.\n",
    "input_size  = 900\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.MSELoss()\n",
    "# optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer    =torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-3)\n",
    "max_epochs  = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model.\n",
    "start=time.perf_counter()\n",
    "model = train_and_test(model, train_data_gen,labels, test_data_gen,labels_test, criterion, optimizer, max_epochs)\n",
    "# model=train(model, train_data_gen,labels, criterion, optimizer)\n",
    "print(model)\n",
    "print('Total time is {} Second'.format(time.perf_counter()-start))\n",
    "#torch.save(model,'without_one_hot_encoding_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Away one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_d.shape torch.Size([2000, 900])\n",
      "train_label.shape torch.Size([2000, 1])\n",
      "test_d.shape torch.Size([100, 900])\n",
      "test_label.shape torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_d=training_data4[0:2000]\n",
    "# train_d=np,atrain_d,\n",
    "train_d=torch.from_numpy(train_d)\n",
    "print('train_d.shape',train_d.shape)\n",
    "train_label=coded_l1[0:2000,1].reshape(-1,1)\n",
    "# np.concatenate((),axis=1)\n",
    "train_label=torch.from_numpy(train_label)\n",
    "print('train_label.shape',train_label.shape)\n",
    "test_d=training_data4[2000:2100]\n",
    "test_d=torch.from_numpy(test_d)\n",
    "print('test_d.shape',test_d.shape)\n",
    "test_label=coded_l1[2000:2100,1]\n",
    "test_label=torch.from_numpy(test_label)\n",
    "print('test_label.shape',test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 / 100] loss: 100.2083740234375, acc 2.4%\n",
      "[Epoch 2 / 100] loss: 104.42540740966797, acc 4.7%\n",
      "[Epoch 3 / 100] loss: 15.651537895202637, acc 10.0%\n",
      "[Epoch 4 / 100] loss: 54.869449615478516, acc 15.15%\n",
      "[Epoch 5 / 100] loss: 0.1483779102563858, acc 18.5%\n",
      "[Epoch 6 / 100] loss: 0.03042624518275261, acc 24.099999999999998%\n",
      "[Epoch 7 / 100] loss: 0.00032884188112802804, acc 28.199999999999996%\n",
      "[Epoch 8 / 100] loss: 21.20081329345703, acc 31.2%\n",
      "[Epoch 9 / 100] loss: 0.3162119686603546, acc 22.45%\n",
      "[Epoch 10 / 100] loss: 0.7997596859931946, acc 1.3%\n",
      "[Epoch 11 / 100] loss: 798.501708984375, acc 1.6%\n",
      "[Epoch 12 / 100] loss: 586.9741821289062, acc 2.25%\n",
      "[Epoch 13 / 100] loss: 89.3946762084961, acc 1.2%\n",
      "[Epoch 14 / 100] loss: 2.426917791366577, acc 1.95%\n",
      "[Epoch 15 / 100] loss: 1.4314595460891724, acc 1.7000000000000002%\n",
      "[Epoch 16 / 100] loss: 18.80291748046875, acc 0.8500000000000001%\n",
      "[Epoch 17 / 100] loss: 15.917948722839355, acc 1.15%\n",
      "[Epoch 18 / 100] loss: 12.829432487487793, acc 0.5499999999999999%\n",
      "[Epoch 19 / 100] loss: 32.182350158691406, acc 1.05%\n",
      "[Epoch 20 / 100] loss: 0.8873106241226196, acc 1.0%\n",
      "[Epoch 21 / 100] loss: 652.3005981445312, acc 1.0%\n",
      "[Epoch 22 / 100] loss: 4340.501953125, acc 0.8%\n",
      "[Epoch 23 / 100] loss: 898.725830078125, acc 0.5499999999999999%\n",
      "[Epoch 24 / 100] loss: 1055.1568603515625, acc 1.3%\n",
      "[Epoch 25 / 100] loss: 531.6319580078125, acc 0.75%\n",
      "[Epoch 26 / 100] loss: 766.030029296875, acc 0.8%\n",
      "[Epoch 27 / 100] loss: 78.8438491821289, acc 1.05%\n",
      "[Epoch 28 / 100] loss: 317.1083068847656, acc 1.2%\n",
      "[Epoch 29 / 100] loss: 0.023965604603290558, acc 0.8%\n",
      "[Epoch 30 / 100] loss: 1839.7457275390625, acc 0.8%\n",
      "[Epoch 31 / 100] loss: 14.486708641052246, acc 0.7000000000000001%\n",
      "[Epoch 32 / 100] loss: 1.1321156024932861, acc 0.44999999999999996%\n",
      "[Epoch 33 / 100] loss: 138.306396484375, acc 0.5499999999999999%\n",
      "[Epoch 34 / 100] loss: 115.81382751464844, acc 0.6%\n",
      "[Epoch 35 / 100] loss: 156.10838317871094, acc 0.8999999999999999%\n",
      "[Epoch 36 / 100] loss: 213.69886779785156, acc 0.65%\n",
      "[Epoch 37 / 100] loss: 910.935791015625, acc 0.6%\n",
      "[Epoch 38 / 100] loss: 88.46884155273438, acc 0.95%\n",
      "[Epoch 39 / 100] loss: 7.327762603759766, acc 1.0999999999999999%\n",
      "[Epoch 40 / 100] loss: 723.91455078125, acc 0.7000000000000001%\n",
      "[Epoch 41 / 100] loss: 149.29063415527344, acc 0.44999999999999996%\n",
      "[Epoch 42 / 100] loss: 280.3247375488281, acc 0.7000000000000001%\n",
      "[Epoch 43 / 100] loss: 5.977219104766846, acc 0.75%\n",
      "[Epoch 44 / 100] loss: 467.9722900390625, acc 0.5499999999999999%\n",
      "[Epoch 45 / 100] loss: 1411.99853515625, acc 0.65%\n",
      "[Epoch 46 / 100] loss: 655.4437866210938, acc 0.7000000000000001%\n",
      "[Epoch 47 / 100] loss: 1495.871826171875, acc 1.0%\n",
      "[Epoch 48 / 100] loss: 100.8434829711914, acc 0.4%\n",
      "[Epoch 49 / 100] loss: 1098.7503662109375, acc 0.5%\n",
      "[Epoch 50 / 100] loss: 1972.404296875, acc 0.7000000000000001%\n",
      "[Epoch 51 / 100] loss: 62.63178253173828, acc 1.15%\n",
      "[Epoch 52 / 100] loss: 443.9532470703125, acc 0.8999999999999999%\n",
      "[Epoch 53 / 100] loss: 0.2549276053905487, acc 0.5499999999999999%\n",
      "[Epoch 54 / 100] loss: 205.7152557373047, acc 0.6%\n",
      "[Epoch 55 / 100] loss: 2.125373125076294, acc 0.5%\n",
      "[Epoch 56 / 100] loss: 47.6137580871582, acc 0.8999999999999999%\n",
      "[Epoch 57 / 100] loss: 548.4580078125, acc 0.7000000000000001%\n",
      "[Epoch 58 / 100] loss: 4.379596710205078, acc 0.3%\n",
      "[Epoch 59 / 100] loss: 11.316071510314941, acc 0.7000000000000001%\n",
      "[Epoch 60 / 100] loss: 1.4138743877410889, acc 0.65%\n",
      "[Epoch 61 / 100] loss: 115.53166198730469, acc 0.8%\n",
      "[Epoch 62 / 100] loss: 108.31484985351562, acc 0.5%\n",
      "[Epoch 63 / 100] loss: 294.3515625, acc 0.7000000000000001%\n",
      "[Epoch 64 / 100] loss: 1210.7000732421875, acc 0.7000000000000001%\n",
      "[Epoch 65 / 100] loss: 36.32881546020508, acc 0.8500000000000001%\n",
      "[Epoch 66 / 100] loss: 3.3769848346710205, acc 0.35000000000000003%\n",
      "[Epoch 67 / 100] loss: 13.278534889221191, acc 0.65%\n",
      "[Epoch 68 / 100] loss: 833.2210083007812, acc 0.6%\n",
      "[Epoch 69 / 100] loss: 3023.483642578125, acc 0.8500000000000001%\n",
      "[Epoch 70 / 100] loss: 7821.24658203125, acc 0.4%\n",
      "[Epoch 71 / 100] loss: 390.22406005859375, acc 0.6%\n",
      "[Epoch 72 / 100] loss: 418.2255554199219, acc 0.6%\n",
      "[Epoch 73 / 100] loss: 127.94661712646484, acc 0.75%\n",
      "[Epoch 74 / 100] loss: 0.2988986074924469, acc 0.75%\n",
      "[Epoch 75 / 100] loss: 43.117828369140625, acc 0.8%\n",
      "[Epoch 76 / 100] loss: 430.2560729980469, acc 0.8500000000000001%\n",
      "[Epoch 77 / 100] loss: 44.709632873535156, acc 0.6%\n",
      "[Epoch 78 / 100] loss: 4474.36669921875, acc 0.8%\n",
      "[Epoch 79 / 100] loss: 821.1393432617188, acc 0.65%\n",
      "[Epoch 80 / 100] loss: 543.317138671875, acc 0.7000000000000001%\n",
      "[Epoch 81 / 100] loss: 220.873046875, acc 0.7000000000000001%\n",
      "[Epoch 82 / 100] loss: 390.2308349609375, acc 0.25%\n",
      "[Epoch 83 / 100] loss: 58.20315170288086, acc 0.5%\n",
      "[Epoch 84 / 100] loss: 1434.7904052734375, acc 0.8%\n",
      "[Epoch 85 / 100] loss: 34.337791442871094, acc 0.7000000000000001%\n",
      "[Epoch 86 / 100] loss: 1.2470277547836304, acc 1.0%\n",
      "[Epoch 87 / 100] loss: 1809.490966796875, acc 0.6%\n",
      "[Epoch 88 / 100] loss: 230.3189239501953, acc 0.65%\n",
      "[Epoch 89 / 100] loss: 0.1261308342218399, acc 0.7000000000000001%\n",
      "[Epoch 90 / 100] loss: 5205.78466796875, acc 0.6%\n",
      "[Epoch 91 / 100] loss: 31.074731826782227, acc 0.95%\n",
      "[Epoch 92 / 100] loss: 3.830700159072876, acc 0.65%\n",
      "[Epoch 93 / 100] loss: 90.41773986816406, acc 0.35000000000000003%\n",
      "[Epoch 94 / 100] loss: 1757.7081298828125, acc 0.5499999999999999%\n",
      "[Epoch 95 / 100] loss: 13161.9912109375, acc 0.44999999999999996%\n",
      "[Epoch 96 / 100] loss: 198.22293090820312, acc 0.7000000000000001%\n",
      "[Epoch 97 / 100] loss: 3732.869140625, acc 0.8999999999999999%\n",
      "[Epoch 98 / 100] loss: 5518.0361328125, acc 0.6%\n",
      "[Epoch 99 / 100] loss: 2.6394155025482178, acc 0.5499999999999999%\n",
      "[Epoch 100 / 100] loss: 74.52367401123047, acc 0.65%\n",
      "SimpleRNN(\n",
      "  (rnn): RNN(900, 500, batch_first=True)\n",
      "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
      ")\n",
      "Total time is 4982.7405209960125 Second\n"
     ]
    }
   ],
   "source": [
    "# Setup the training and test data generators.\n",
    "batch_size     = 100\n",
    "train_data_gen = train_d\n",
    "test_data_gen  = test_d\n",
    "labels   =     train_label\n",
    "labels_test=test_label\n",
    "# print('train_data_gen.shape',train_data_gen.shape)\n",
    "# print('labels.shape',labels.shape)\n",
    "# Setup the RNN and training settings.\n",
    "input_size  = 900\n",
    "hidden_size = 500\n",
    "output_size = 1\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.MSELoss()\n",
    "# optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "optimizer    =torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-3)\n",
    "max_epochs  = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Train the model.\n",
    "start=time.perf_counter()\n",
    "model = train_and_test(model, train_data_gen,labels, test_data_gen,labels_test, criterion, optimizer, max_epochs)\n",
    "# model=train(model, train_data_gen,labels, criterion, optimizer)\n",
    "print(model)\n",
    "print('Total time is {} Second'.format(time.perf_counter()-start))\n",
    "#torch.save(model,'without_one_hot_encoding_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
